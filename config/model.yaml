model:
  # GitHub Models endpoint (OpenAI-compatible)
  endpoint: "https://models.inference.ai.azure.com"
  
  # Model name (available on GitHub Models)
  # Options: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
  model_name: "gpt-4o"
  
  # API key from environment variable
  api_key: null  # Set via GITHUB_TOKEN env var
  
  # Generation parameters
  temperature: 0.2  # Lowered from 0.7 - reduces variance, stabilizes output
  max_tokens: 1024
  
  # Scoring configuration
  max_segments_to_score: 50
  # Rank-based selection: take top_k clips; absolute_min_score is the sanity floor
  top_k: 10
  absolute_min_score: 30
  
  # Rate limiting and batching
  batch_size: 6  # Segments per API call
  pre_filter_count: 20  # Max candidates after heuristic filtering
  inter_request_delay: 1.5  # Seconds between API calls
  max_cooldown_threshold: 60  # If retry-after > this, stop pipeline
  
  # ========================================
  # ENHANCED VIRAL DETECTION PIPELINE (NEW)
  # ========================================
  
  # Enable/disable enhanced pipeline (default: true)
  use_enhanced_pipeline: true
  
  # Psychological Virality Scoring
  # NOTE: threshold is no longer a hard gate; psychological_score is used as a
  # weighted component (25%) in the composite final_score.
  psychological_threshold: 65.0  # kept for reference / logging only
  top_clips: 10  # Number of top clips to return after full analysis
  
  # Hook Quality (First 7 Seconds)
  min_hook_score: 5.0  # Minimum hook score (0-10) to pass filter
  
  # Semantic Deduplication
  similarity_threshold: 0.85  # Cosine similarity threshold (0-1) for duplicates
  
  # LLM Scoring Integration
  use_llm_scoring: true  # Whether to use LLM scoring in Stage 2 (deep analysis)
